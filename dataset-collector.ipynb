{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from utils import download_diffusiondb\n",
    "from clip_dataset_encoder import encode_dataframe\n",
    "import os\n",
    "from image_noiser import ImageNoiser\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download parquet files and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download parquet files\n",
    "!python scripts/dataset_parquet_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hord_score_df = pd.read_parquet('parquets/hord_diffusiondb_scores.parquet')\n",
    "train_split_df = pd.read_parquet('parquets/train_split.parquet')\n",
    "validate_split_df = pd.read_parquet('parquets/validate_split.parquet')\n",
    "prepared_hord_df = pd.read_parquet('parquets/prepared_hord_diffusion_dataset.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download images in shards, until it exceeds needed amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES_TO_COLLECT = 1000\n",
    "part_id = 0\n",
    "collected_samples = sum([len(files) for r, d, files in os.walk(\"images\")])\n",
    "df_latents = pd.DataFrame(columns=prepared_hord_df.columns)\n",
    "print(f'collected images so far: {collected_samples}')\n",
    "while collected_samples < NUM_IMAGES_TO_COLLECT:\n",
    "    part_id += 1\n",
    "    download_diffusiondb.main(index=part_id, range_max=None, output='images/', unzip=True, large=True)\n",
    "    for image_name in os.listdir(f'images/part-{part_id:06}'):\n",
    "        matching_rows = prepared_hord_df[prepared_hord_df['image_name'] == image_name]\n",
    "        if len(matching_rows) == 0:\n",
    "            os.remove(f'images/part-{part_id:06}/{image_name}')\n",
    "        else:\n",
    "            df_latents = pd.concat([df_latents, matching_rows], ignore_index=True)\n",
    "    collected_samples += len(os.listdir(f'images/part-{part_id:06}'))\n",
    "    print(f'collected images so far: {collected_samples}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create latent representations of all images in images/ folder and save them in latent_images/ in the corresponding shards\n",
    "#image_processor is warning about latents range, but in expected range [0, 1] latents are not approximated correctly\n",
    "if not os.path.exists('latent_images'):\n",
    "    os.mkdir('latent_images')\n",
    "noise_timestep = 10 #Create latents at step 10 for now, may reduce or increase later based on results of experiments\n",
    "scheduler_timesteps = 20\n",
    "image_noiser = ImageNoiser()\n",
    "for part_idx in range(1, part_id+1):\n",
    "    if not os.path.exists(f'latent_images/part-{part_idx:06}'):\n",
    "        os.mkdir(f'latent_images/part-{part_idx:06}')\n",
    "    for image_name in os.listdir(f'images/part-{part_idx:06}'):\n",
    "        img = Image.open(f'images/part-{part_idx:06}/{image_name}')\n",
    "        #encode an image to a latent\n",
    "        img_latent = image_noiser.encode_image(img)\n",
    "        #Add noise to that latent with DPMSolverScheduler\n",
    "        noisy_latent = image_noiser.add_noise_to_latent(img_latent, noise_timestep, scheduler_timesteps)\n",
    "        #approximate that latent with matrix multiplication which works for some fucking reason lmao\n",
    "        noisy_latent_approx = image_noiser.approx_latent(noisy_latent)\n",
    "        #Save that image in latent_images/ folder with the same part_id\n",
    "        noisy_latent_approx.save(f'latent_images/part-{part_idx:06}/{image_name.split(\".\")[0]}.png')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_to_latent(row):\n",
    "    return f\"latent_images/part-{row['part_id']:06d}/{row['image_name'].split('.')[0]}.png\"\n",
    "df_latents['latent_image_path'] = df_latents.apply(create_path_to_latent, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latents_train, df_latents_val = np.split(df_latents.sample(frac=1, random_state=42), [int(0.8*len(df_latents))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_latents.to_parquet('parquets/latents-approximated.parquet', index=False)\n",
    "df_latents_train.to_csv('csvs/clip_train.csv', sep=\"\\t\", index=False)\n",
    "df_latents_train.to_parquet('parquets/latents-approximated-train.parquet', index=False)\n",
    "df_latents_val.to_csv('csvs/clip_val.csv', sep=\"\\t\", index=False)\n",
    "df_latents_val.to_parquet('parquets/latents-approximated-val.parquet', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OPTIONAL*: Run CLIP training, change config if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./scripts/run_clip_train.sh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trained CLIP, run dataframe encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_dataframe(\"hf-hub:Outrun32/CLIP-ViT-B-16-noise-tuned\",df_latents,clip_batch_size=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MLP training on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python artifact_estimator/train.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, go to demo.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IntroToML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
